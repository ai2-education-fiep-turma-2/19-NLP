{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante essa aula vamos explorar um pouco os tipos de tarefas que podemos resolver utilizando os conceitos aprendidos nas últimas 2 aulas.\n",
    "\n",
    "A primeira tarefa será de **classificacão de texto**, similar à tarefa explorada na primeira semana.\n",
    "\n",
    "A tarefa consiste em analisar uma notícia e classificá-la dentre um conjunto possível de temas, por exemplo, *tecnologia*, *esporte*, etc.\n",
    "\n",
    "Para isto, vamos utilizar o dataset liberado pela [BBC](http://mlg.ucd.ie/datasets/bbc.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/text_classification/bbc-text.csv\")\n",
    "\n",
    "num_classes = len(data.category.value_counts())\n",
    "\n",
    "\n",
    "training_proportion = 0.9\n",
    "\n",
    "training_index = int(training_proportion * len(data))\n",
    "\n",
    "train_data = data.iloc[:training_index,:]\n",
    "test_data = data.iloc[training_index:, :]\n",
    "\n",
    "train_text = train_data.loc[:, 'text'].tolist()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas últimas aulas, apesar de utilizarmos embeddings em todas as tarefas, estávamos os aprendendo do zero baseando-se na base de treinamento.\n",
    "\n",
    "Na verdade, não há necessidade disso porque já existe uma variedade muito grande de embeddings pré-treinados que podem ser utilizados.\n",
    "\n",
    "Para este exercício vamos utilizar o [GloVe](https://nlp.stanford.edu/projects/glove/), mas existem muitas opcões de embeddings pré-treinados, inclusive em português.\n",
    "\n",
    "Antes de rodar as próximas linhas, baixe o arquivo [glove.6B.50d.txt](https://github.com/uclnlp/inferbeddings/blob/master/data/glove/glove.6B.50d.txt.gz)\n",
    "\n",
    "Vamos ler o conteúdo do arquivo com embeddings pré-treinados, para posteriormente utilizá-los na nossa camada de Embedding. O propósito da próxima célula é determinar o vocabulário para a nossa base de treinamento, e carregar os embeddings para todas as palavras encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "max_vocab = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_vocab)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "indexes = min(max_vocab, len(tokenizer.word_index))\n",
    "\n",
    "num_emb = 0\n",
    "cons_keys = []\n",
    "for word in tokenizer.word_index.keys():\n",
    "    cons_keys.append(word)\n",
    "    num_emb += 1\n",
    "    if num_emb == max_vocab:\n",
    "        break\n",
    "\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('data/text_classification/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in cons_keys:\n",
    "        coefs = np.array(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((max_vocab, 50))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i>=max_vocab:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[10:13])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora faca o processamento restante necessário, including padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sentence_size = # Seu parametro\n",
    "\n",
    "dataset_train_sequences = #TOkenize a base de treinamento \n",
    "dataset_test_sequences = #TOkenize a base de teste\n",
    "\n",
    "padded_train = # Realize Padding na base de treinamento\n",
    "padded_test = # Realize Padding na base de teste\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# 50 é o número de dimensoes do embedding (definido pelo arquivo que baixamos)\n",
    "#trainable = False significa que esses pesos nao sao atualizados durante o treinamento.\n",
    "model.add(Embedding(max_vocab, 50, weights=[embedding_matrix], input_length=sentence_size, trainable=False))\n",
    "#Resto da sua arquitetura\n",
    "\n",
    "optimizer = # otimizador\n",
    "loss = #Loss\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.utils as ku\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit_transform(train_data.loc[:, 'category'])\n",
    "\n",
    "label_train = train_data.loc[:, ['category']].apply(LabelEncoder().fit_transform).values\n",
    "label_test = test_data.loc[:, ['category']].apply(LabelEncoder().fit_transform).values\n",
    "\n",
    "label_train = ku.to_categorical(label_train, num_classes = num_classes)\n",
    "label_test = ku.to_categorical(label_test, num_classes = num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded_train,label_train, epochs = num_epochs, validation_data=(padded_test,label_test), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implemente essa funcao para pegar uma lista de textos e retornar o assunto de cada um\n",
    "def predict_topic(text):\n",
    "    #Pre processamento\n",
    "    topic = np.argmax(model.predict(proc_text), axis=1)\n",
    "    topic = labelencoder.inverse_transform(topic)\n",
    "    return topic\n",
    "\n",
    "predict_topic(['A lot of data centers',\"A lot of stock prices and financial data\", 'A lot of goals scored'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já somos experts em classificar sentimentos em textos, vamos considerar uma tarefa diferente mas igualmente importante: **Traducão**.\n",
    "\n",
    "Como realizar traducões entre duas linguas requer modelos grandes e um grande corpus de treinamento, vamos considerar uma tarefa de \"traducão\" mais simples.\n",
    "\n",
    "A tarefa que consideraremos é a traducão de uma data em texto livre para um formato mais palatável para o computador. Alguns exemplos de traducão estão abaixo:\n",
    "\n",
    "`9 may 1998 -> 1998-05-09\n",
    "10.11.19 -> 2019-11-10\n",
    "9/10/70 -> 1970-09-10\n",
    "saturday april 28 1990 -> 1990-04-28\n",
    "thursday january 26 1995 -> 1995-01-26\n",
    "monday march 7 1983 -> 1983-03-07`\n",
    "\n",
    "Para realizar esta tarefa, modelaremos o problema de uma forma um pouco diferente. Para a criacão de nosso vocabulário, designaremos um código para cada caractere (não cada palavra), formando um vocabulario de origem e um de destino.\n",
    "\n",
    "Seu modelo irá ler um conjunto de caracteres de entrada de tamanho `n`, processá-lo e dar como saída um conjunto de caracteres de tamanho `10`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/translation/dates_dataset.csv', header=None)\n",
    "train_index = int(0.9 * len(data))\n",
    "train_data = data[:train_index]\n",
    "test_data = data[train_index:]\n",
    "\n",
    "#Construindo o tokenizer para cada caracter com o parametro char_level = True\n",
    "tok_origem = Tokenizer(char_level=True)\n",
    "tok_destino = Tokenizer(char_level = True)\n",
    "\n",
    "tok_origem.fit_on_texts(train_data.loc[:,0])\n",
    "tok_destino.fit_on_texts(train_data.loc[:,1])\n",
    "\n",
    "seq_train = tok_origem.texts_to_sequences(train_data.loc[:,0])\n",
    "seq_test = tok_origem.texts_to_sequences(test_data.loc[:,0])\n",
    "\n",
    "label_train = np.array(tok_destino.texts_to_sequences(train_data.loc[:,1]))\n",
    "label_test = np.array(tok_destino.texts_to_sequences(test_data.loc[:,1]))\n",
    "\n",
    "label_train = ku.to_categorical(label_train, num_classes = len(tok_destino.word_index)+1)\n",
    "label_test = ku.to_categorical(label_test, num_classes = len(tok_destino.word_index)+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_orig = 30\n",
    "max_size_dest = 10\n",
    "\n",
    "padded_train = pad_sequences(seq_train, maxlen=max_size_orig, padding='post', truncating='post')\n",
    "padded_test = pad_sequences(seq_test, maxlen=max_size_orig, padding='post', truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Lambda, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tok_origem.word_index)+1, 50,  input_length=max_size_orig))\n",
    "#Sua Arquitetura\n",
    "model.add(Dense(len(tok_destino.word_index)+1, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = #Optimizer\n",
    "loss = #Loss\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.fit(padded_train,label_train, epochs = num_epochs, validation_data=(padded_test,label_test), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer uma funcao que pega uma data no formato de origem e a transforma para o formato destino\n",
    "def predict_dates(dates):\n",
    "    #Seu codigo aqui\n",
    "    return converted\n",
    "                           \n",
    "predict_dates(['august 26 1975', 'friday july 12 1991', '10 sep 1975'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o último problema a ser resolvido, vamos considerar um problema um pouco mais exótico, semelhante ao problema de geracão de palavras estudado na segunda aula.\n",
    "\n",
    "\n",
    "Para esta tarefa geraremos **nomes de Dinossauros**.\n",
    "\n",
    "Nomes de dinossauros têm uma certa semelhanca, por exemplo:\n",
    "\n",
    "`\n",
    "Abdallahsaurus\n",
    "Abelisaurus\n",
    "Abrictosaurus\n",
    "`\n",
    "\n",
    "Portanto, vamos treinar um modelo que completa o nome de um dinossauro!\n",
    "\n",
    "Para isto, novamente vamos gerar um token para cara caractere. O modelo deve ser treinado recebendo um nome parcial e o completando, por exemplo:\n",
    "\n",
    "`x = 'Abda', y = 'Abdallahsaurus'\n",
    "x = 'Abelisa', y = 'Abelisaurus'\n",
    "`\n",
    "\n",
    "O modelo pode ser treinado com a base de treinamento sendo gerada similarmente ao problema de gerar sonetos, mas adicionando um token \"\\n\" à saída para indicar o final do nome do Dinossauro:\n",
    "\n",
    "\n",
    "`x = 'A', y = 'Aardonyx\\n'\n",
    "x = 'Aa', y = 'Aardonyx\\n'\n",
    "x = 'Aar', y = 'Aardonyx\\n'\n",
    "...\n",
    "x = 'Aardony', y = 'Aardonyx\\n'\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list =[]\n",
    "largest_name = 0\n",
    "with open(\"data/text_generation/dinos.txt\", \"r\") as a_file:\n",
    "  for line in a_file:\n",
    "    largest_name = max(largest_name, len(line))\n",
    "    for i in range(len(line)-1):\n",
    "        row_list.append([line[:i+1], line])\n",
    "\n",
    "data = pd.DataFrame(row_list, columns = ['x', 'y'])\n",
    "data.head(12)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "training_index = int(0.9*len(data))\n",
    "train_data = data.loc[:training_index,:]\n",
    "test_data = data.loc[training_index:, :]\n",
    "\n",
    "#Faća agora todo o processo de pre-processamento ate o treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(words_num+1, 50,  input_length=largest_name))\n",
    "#Sua arquitetura\n",
    "model.add(Dense(words_num+1, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = #Optimizer\n",
    "loss = #Loss\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.fit(padded_train_seq,train_labels, epochs = num_epochs, validation_data=(padded_test_seq,test_labels), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dino(prefixos):\n",
    "    seqs = tok.texts_to_sequences(prefixos)\n",
    "    padded = pad_sequences(seqs, maxlen=largest_name, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded)\n",
    "    predictions = np.argmax(predictions,axis=2)\n",
    "    converted = tok.sequences_to_texts(predictions)\n",
    "    converted = [\"\".join(x.split()) for x in converted]\n",
    "    final = [x.split('\\n')[0] for x in converted]\n",
    "    final = [prefixos[i]+final[i][len(prefixos[i]):] for i in range(len(final))]\n",
    "    return final\n",
    "             \n",
    "predict_dino(['test', 'brasil', 'fiep'])                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
